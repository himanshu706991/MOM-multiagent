{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c73965",
   "metadata": {},
   "source": [
    "\n",
    "# Agentic RAG End-to-End Demo (PDF vs SQL routing)\n",
    "\n",
    "**Purpose:** A reproducible, self-contained Jupyter notebook that demonstrates an *agentic* RAG pipeline:\n",
    "- Data preparation (SQL sample DB + sample PDF-like docs)\n",
    "- Ingestion & chunking\n",
    "- Embedding & vector index creation (FAISS with `sentence-transformers` embeddings)\n",
    "- Separate VectorDBs: `pdf_index` and `sql_schema_index`\n",
    "- Router agent (embedding-based; optional OpenAI LLM router)\n",
    "- PDF retrieval pipeline (retrieve chunks → summarize/polish by LLM or fallback)\n",
    "- SQL pipeline (retrieve schema snippets → generate SQL with LLM or fallback → execute → polish)\n",
    "- Final orchestration that shows the difference between Traditional RAG and Agentic RAG\n",
    "\n",
    "**Notes:** This notebook uses local embeddings (`sentence-transformers/all-MiniLM-L6-v2`) and FAISS so it runs offline. Optional OpenAI functionality is included (router, SQL generator, polish) — set `OPENAI_API_KEY` in your environment to enable those steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6eef8",
   "metadata": {},
   "source": [
    "## 0) Install (one-time) and environment notes\n",
    "\n",
    "Run the following in your environment if you don't have the required packages:\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers faiss-cpu numpy pandas python-dotenv openai reportlab python-docx PyMuPDF\n",
    "# If faiss-cpu is not available for your platform, use chromadb instead.\n",
    "```\n",
    "\n",
    "Set `OPENAI_API_KEY` env var if you want to use OpenAI steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb8aba",
   "metadata": {},
   "source": [
    "## 1) Imports and helper utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e760ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports & helper functions\n",
    "import os, sqlite3, json, uuid, textwrap, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# FAISS (vector index)\n",
    "import faiss\n",
    "\n",
    "# small helpers\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10))\n",
    "\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac2831",
   "metadata": {},
   "source": [
    "## 2) Create sample SQL database (SQLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a small demo SQLite DB with multiple tables and relationships\n",
    "DB_PATH = \"/mnt/data/demo_claims.db\"\n",
    "if os.path.exists(DB_PATH):\n",
    "    os.remove(DB_PATH)\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript(\\\"\\\"\\\"\n",
    "CREATE TABLE policy_header (\n",
    "    policy_id INTEGER PRIMARY KEY,\n",
    "    policy_holder TEXT,\n",
    "    start_date TEXT,\n",
    "    end_date TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE claim_header (\n",
    "    claim_id INTEGER PRIMARY KEY,\n",
    "    policy_id INTEGER,\n",
    "    loss_date TEXT,\n",
    "    claim_status TEXT,\n",
    "    loss_amount REAL,\n",
    "    FOREIGN KEY(policy_id) REFERENCES policy_header(policy_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE transaction_log (\n",
    "    transaction_id INTEGER PRIMARY KEY,\n",
    "    claim_id INTEGER,\n",
    "    payment_date TEXT,\n",
    "    amount_paid REAL,\n",
    "    FOREIGN KEY(claim_id) REFERENCES claim_header(claim_id)\n",
    ");\n",
    "\\\"\\\"\\\")\n",
    "\n",
    "cur.executemany(\"INSERT INTO policy_header(policy_id, policy_holder, start_date, end_date) VALUES (?,?,?,?)\", [\n",
    "    (1, \"Acme Corp\", \"2023-01-01\", \"2024-12-31\"),\n",
    "    (2, \"John Doe\", \"2022-05-10\", \"2025-05-09\"),\n",
    "])\n",
    "cur.executemany(\"INSERT INTO claim_header(claim_id, policy_id, loss_date, claim_status, loss_amount) VALUES (?,?,?,?,?)\", [\n",
    "    (101, 1, \"2024-01-20\", \"open\", 15000.0),\n",
    "    (102, 2, \"2024-03-05\", \"closed\", 4000.0),\n",
    "    (103, 1, \"2024-06-10\", \"closed\", 2200.0),\n",
    "])\n",
    "cur.executemany(\"INSERT INTO transaction_log(transaction_id, claim_id, payment_date, amount_paid) VALUES (?,?,?,?)\", [\n",
    "    (1001, 102, \"2024-03-10\", 4000.0),\n",
    "    (1002, 103, \"2024-06-15\", 2200.0)\n",
    "])\n",
    "conn.commit()\n",
    "print(\"Created demo DB at\", DB_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc025a35",
   "metadata": {},
   "source": [
    "## 3) Create sample document texts (PDF-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll use simple .txt files to represent PDF contents for this demo\n",
    "DOC_DIR = \"/mnt/data/demo_docs\"\n",
    "os.makedirs(DOC_DIR, exist_ok=True)\n",
    "\n",
    "policy_doc = \\\"\\\"\\\"Policy Document - Claims Process\n",
    "1. Claim filing: The claimant should notify within 30 days.\n",
    "2. Initial review: Documents collected and verified.\n",
    "3. Field inspection: If damage is physical, an inspector visits within 5 days.\n",
    "4. Approval & payment: Once approved, payment processed within 10 business days.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "notes_doc = \\\"\\\"\\\"Claims handling - internal notes\n",
    "- For complex claims escalate to Senior Adjuster.\n",
    "- Authentication & fraud checks necessary for claims > $10,000.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "with open(os.path.join(DOC_DIR, \"policy_claims.txt\"), \"w\") as f:\n",
    "    f.write(policy_doc)\n",
    "with open(os.path.join(DOC_DIR, \"notes_claims.txt\"), \"w\") as f:\n",
    "    f.write(notes_doc)\n",
    "\n",
    "print(\"Wrote demo docs to\", DOC_DIR)\n",
    "for p in os.listdir(DOC_DIR):\n",
    "    print(\"-\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb21008",
   "metadata": {},
   "source": [
    "## 4) Chunking function (split long text into overlapping chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(text: str, chunk_size_words: int = 150, overlap_words: int = 30) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i+chunk_size_words])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size_words - overlap_words\n",
    "    return chunks\n",
    "\n",
    "# quick test\n",
    "print(chunk_text(' '.join(['word']*400), chunk_size_words=100, overlap_words=20)[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7fbc0",
   "metadata": {},
   "source": [
    "## 5) Load embedding model (sentence-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "print(\"Loading embedding model:\", EMBED_MODEL_NAME)\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "def embed_texts(texts: List[str]):\n",
    "    embs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=True)\n",
    "    return embs\n",
    "print(\"Embedding model ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619cd96",
   "metadata": {},
   "source": [
    "## 6) FAISS index wrapper (store embeddings + metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaissIndex:\n",
    "    def __init__(self, dim: int):\n",
    "        self.dim = dim\n",
    "        self.index = faiss.IndexFlatIP(dim)  # inner product on normalized vectors => cosine similarity\n",
    "        self.metadatas = []\n",
    "\n",
    "    def add(self, embeddings: np.ndarray, metadatas: List[dict]):\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        self.metadatas.extend(metadatas)\n",
    "\n",
    "    def search(self, query_emb: np.ndarray, top_k: int = 5):\n",
    "        if query_emb.ndim == 1:\n",
    "            query_emb = query_emb.reshape(1, -1)\n",
    "        D, I = self.index.search(query_emb.astype('float32'), top_k)\n",
    "        out = []\n",
    "        for ids, scores in zip(I, D):\n",
    "            hits = []\n",
    "            for idx, score in zip(ids, scores):\n",
    "                if idx < 0 or idx >= len(self.metadatas):\n",
    "                    continue\n",
    "                md = dict(self.metadatas[idx])\n",
    "                md['_score'] = float(score)\n",
    "                md['_id'] = int(idx)\n",
    "                hits.append(md)\n",
    "            out.append(hits)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a1a13",
   "metadata": {},
   "source": [
    "## 7) Ingest docs → chunk → embed → build `pdf_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e131064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read docs, chunk, embed, add to index\n",
    "pdf_index = None\n",
    "all_chunks = []\n",
    "for fname in os.listdir(DOC_DIR):\n",
    "    path = os.path.join(DOC_DIR, fname)\n",
    "    with open(path, 'r') as f:\n",
    "        txt = f.read()\n",
    "    chunks = chunk_text(txt, chunk_size_words=120, overlap_words=30)\n",
    "    embs = embed_texts(chunks)\n",
    "    if pdf_index is None:\n",
    "        pdf_index = FaissIndex(dim=embs.shape[1])\n",
    "    metas = [{\"source\":\"pdf\", \"doc\": fname, \"chunk\": i, \"text\": chunks[i]} for i in range(len(chunks))]\n",
    "    pdf_index.add(embs, metas)\n",
    "    all_chunks.extend(metas)\n",
    "\n",
    "print(\"PDF index built — total chunks:\", len(pdf_index.metadatas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf439696",
   "metadata": {},
   "source": [
    "## 8) Extract SQL schema metadata → create textual docs → embed → build `sql_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_schema_docs(sql_conn: sqlite3.Connection) -> List[dict]:\n",
    "    cur = sql_conn.cursor()\n",
    "    cur.execute(\\\"\\\"\\\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\\\"\\\"\\\")\n",
    "    tables = [r[0] for r in cur.fetchall()]\n",
    "    docs = []\n",
    "    for table in tables:\n",
    "        cur.execute(f\"PRAGMA table_info({table});\")\n",
    "        cols = cur.fetchall()\n",
    "        col_lines = []\n",
    "        for cid, name, ctype, notnull, dflt, pk in cols:\n",
    "            # sample values (non-exhaustive)\n",
    "            try:\n",
    "                cur.execute(f\"SELECT {name} FROM {table} WHERE {name} IS NOT NULL LIMIT 3;\")\n",
    "                sample = [str(r[0]) for r in cur.fetchall()]\n",
    "            except Exception:\n",
    "                sample = []\n",
    "            col_lines.append(f\"- {name} ({ctype}) sample: {', '.join(sample)}\")\n",
    "        cur.execute(f\"PRAGMA foreign_key_list({table});\")\n",
    "        fkeys = cur.fetchall()\n",
    "        fk_lines = []\n",
    "        for fk in fkeys:\n",
    "            # fk format: (id, seq, table, from, to, on_update, on_delete, match)\n",
    "            fk_lines.append(f\"- {fk[3]}.{fk[4]} -> {fk[2]}.{fk[4]}\")\n",
    "        text = f\\\"Table: {table}\\\\nColumns:\\\\n\\\" + \\\"\\\\n\\\".join(col_lines)\n",
    "        if fk_lines:\n",
    "            text += \"\\\\nForeignKeys:\\\\n\" + \"\\\\n\".join(fk_lines)\n",
    "        docs.append({\"table\": table, \"text\": text})\n",
    "    return docs\n",
    "\n",
    "sql_schema_docs = extract_schema_docs(conn)\n",
    "schema_texts = [d[\"text\"] for d in sql_schema_docs]\n",
    "schema_embs = embed_texts(schema_texts)\n",
    "sql_index = FaissIndex(dim=schema_embs.shape[1])\n",
    "metas = [{\"source\":\"sql_schema\", \"table\": sql_schema_docs[i][\"table\"], \"text\": schema_texts[i]} for i in range(len(schema_texts))]\n",
    "sql_index.add(schema_embs, metas)\n",
    "print(\"Built SQL schema index for tables:\", [m['table'] for m in metas])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64757152",
   "metadata": {},
   "source": [
    "## 9) Router agent — embedding classifier (fast offline) + optional OpenAI router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f319511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prototype texts for 'pdf' vs 'sql' routing\n",
    "proto_texts = [\n",
    "    (\"pdf\", \"Where is the claims filing process described in the policy document?\"),\n",
    "    (\"sql\", \"Give total paid amount per policy for 2024; aggregate by policy\")\n",
    "]\n",
    "proto_embs = embed_texts([p[1] for p in proto_texts])\n",
    "proto_labels = [p[0] for p in proto_texts]\n",
    "\n",
    "def route_with_embedding(query: str) -> str:\n",
    "    q_emb = embed_texts([query])[0]\n",
    "    sims = [cosine_sim(q_emb, pe) for pe in proto_embs]\n",
    "    best = int(np.argmax(sims))\n",
    "    return proto_labels[best]\n",
    "\n",
    "# Optional OpenAI router (requires OPENAI_API_KEY)\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "def route_with_openai(query: str) -> str:\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
    "    import openai\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    prompt = f\\\"\\\"\\\"You are a router. Decide ONLY one label for the user question: 'pdf' or 'sql'. Provide only the label.\n",
    "User question: {query}\n",
    "\\\"\\\"\\\"\n",
    "    resp = openai.ChatCompletion.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\",\"content\":prompt}], temperature=0)\n",
    "    label = resp.choices[0].message.content.strip().lower()\n",
    "    return label\n",
    "\n",
    "# quick tests\n",
    "print(\"Route test 1:\", route_with_embedding(\"How does the claims filing process work?\"))\n",
    "print(\"Route test 2:\", route_with_embedding(\"Show total paid amount per policy for 2024\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8e244",
   "metadata": {},
   "source": [
    "## 10) PDF retrieval pipeline (retrieve top chunks → (optional) LLM summarize/polish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdf_pipeline(query: str, top_k: int = 3, use_openai=False) -> str:\n",
    "    q_emb = embed_texts([query])\n",
    "    hits = pdf_index.search(q_emb, top_k=top_k)[0]\n",
    "    context = \"\\n\\n\".join([f\\\"[doc:{h['doc']}|chunk:{h['chunk']}] {h['text']}\\\" for h in hits])\n",
    "    if use_openai and OPENAI_API_KEY:\n",
    "        import openai\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        prompt = f\\\"\\\"\\\"You are an assistant. Use the following context (from documents) to answer the user's question in a concise, client-ready way. Include citations to documents where appropriate.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\\\"\\\"\\\"\n",
    "        resp = openai.ChatCompletion.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\",\"content\":prompt}], temperature=0)\n",
    "        return resp.choices[0].message.content\n",
    "    else:\n",
    "        return \"RETRIEVED (top chunks):\\\\n\\\\n\" + context\n",
    "\n",
    "# demo\n",
    "print(pdf_pipeline(\"How does the claims filing process work?\", top_k=2, use_openai=False)[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d0767",
   "metadata": {},
   "source": [
    "## 11) SQL pipeline — retrieve schema snippets → generate SQL → execute → polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abcac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_relevant_schema(query: str, top_k: int = 3):\n",
    "    q_emb = embed_texts([query])\n",
    "    hits = sql_index.search(q_emb, top_k=top_k)[0]\n",
    "    return hits\n",
    "\n",
    "def generate_sql_fallback(query: str, schema_hits: List[dict]) -> str:\n",
    "    # Simple heuristic SQL generator for demo purposes (handles aggregate payment per policy example)\n",
    "    text = \" \".join([h[\"text\"] for h in schema_hits]).lower()\n",
    "    if \"total\" in query.lower() and (\"amount\" in query.lower() or \"paid\" in query.lower()):\n",
    "        # use known table names from our demo DB\n",
    "        sql = \\\"\\\"\\\"SELECT p.policy_holder, SUM(t.amount_paid) as total_paid\n",
    "FROM transaction_log t\n",
    "JOIN claim_header c ON t.claim_id = c.claim_id\n",
    "JOIN policy_header p ON c.policy_id = p.policy_id\n",
    "WHERE c.loss_date LIKE '2024-%'\n",
    "GROUP BY p.policy_holder\n",
    "ORDER BY total_paid DESC;\\\"\\\"\\\"\n",
    "        return sql\n",
    "    return \"SELECT * FROM claim_header LIMIT 5;\"\n",
    "\n",
    "def generate_sql_with_openai(query: str, schema_snippets: List[str]) -> str:\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
    "    import openai\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    schema_context = \"\\n\\n\".join(schema_snippets)\n",
    "    prompt = f\\\"\\\"\\\"You are an SQL generator for SQLite. Given the schema snippets below, produce a syntactically correct SQL query (ONLY SQL) that answers the user's question. Use only tables/columns from the schema snippets.\\n\\nSchema:\\n{schema_context}\\n\\nUser question: {query}\\n\\nSQL:\\\"\\\"\\\"\n",
    "    resp = openai.ChatCompletion.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\",\"content\":prompt}], temperature=0)\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def execute_sql(conn: sqlite3.Connection, sql: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_sql_query(sql, conn)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame({\"error\":[str(e)], \"sql\":[sql]})\n",
    "\n",
    "def sql_pipeline(query: str, use_openai_sql=False):\n",
    "    schema_hits = retrieve_relevant_schema(query, top_k=5)\n",
    "    schema_snips = [h[\"text\"] for h in schema_hits]\n",
    "    # Choose SQL generator\n",
    "    if use_openai_sql and OPENAI_API_KEY:\n",
    "        sql = generate_sql_with_openai(query, schema_snips)\n",
    "    else:\n",
    "        sql = generate_sql_fallback(query, schema_hits)\n",
    "    df = execute_sql(conn, sql)\n",
    "    # Polishing\n",
    "    if OPENAI_API_KEY:\n",
    "        import openai\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        context = f\\\"SQL:\\\\n{sql}\\\\n\\\\nResults:\\\\n{df.head(20).to_string(index=False)}\\\"\n",
    "        prompt = f\\\"Polish the SQL results into a user-friendly summary. Highlight anomalies and key numbers.\\\\n\\\\n{context}\\\\n\\\\nSummary:\\\"\n",
    "        resp = openai.ChatCompletion.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\",\"content\":prompt}], temperature=0)\n",
    "        polished = resp.choices[0].message.content\n",
    "    else:\n",
    "        polished = df.head(10).to_string(index=False)\n",
    "    return {\"sql\": sql, \"raw\": df, \"polished\": polished}\n",
    "\n",
    "# Demo SQL pipeline\n",
    "demo_q = \"Show total paid amount per policy for claims opened in 2024\"\n",
    "out = sql_pipeline(demo_q, use_openai_sql=False)\n",
    "print(\"Generated SQL:\\\\n\", out[\"sql\"])\n",
    "print(\"\\\\nPolished / result preview:\\\\n\", out[\"polished\"][:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee3d9f",
   "metadata": {},
   "source": [
    "## 12) Orchestration: routing → appropriate pipeline → final output\n",
    "\n",
    "This cell demonstrates the full orchestration and compares **Traditional RAG** (single vector DB search) vs **Agentic RAG** (router → specialized indexes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea3e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def traditional_rag(query: str):\n",
    "    # naive single-index approach: search PDF index and SQL schema index combined (here we simulate mixing by searching pdf_index only)\n",
    "    q_emb = embed_texts([query])\n",
    "    pdf_hits = pdf_index.search(q_emb, top_k=3)[0]\n",
    "    context = \"\\n\\n\".join([f\\\"[doc:{h['doc']}] {h['text']}\\\" for h in pdf_hits])\n",
    "    return {\"route\":\"traditional\", \"context\": context}\n",
    "\n",
    "def agentic_rag(query: str, prefer_openai_router=False, prefer_openai_sql=False, prefer_openai_pdf=False):\n",
    "    # routing\n",
    "    try:\n",
    "        if prefer_openai_router and OPENAI_API_KEY:\n",
    "            route = route_with_openai(query)\n",
    "        else:\n",
    "            route = route_with_embedding(query)\n",
    "    except Exception:\n",
    "        route = route_with_embedding(query)\n",
    "    route = route.lower()\n",
    "    if route == \"pdf\":\n",
    "        answer = pdf_pipeline(query, top_k=3, use_openai=prefer_openai_pdf and bool(OPENAI_API_KEY))\n",
    "        return {\"route\":\"pdf\", \"answer\": answer}\n",
    "    elif route == \"sql\":\n",
    "        sql_out = sql_pipeline(query, use_openai_sql=prefer_openai_sql and bool(OPENAI_API_KEY))\n",
    "        return {\"route\":\"sql\", \"sql\": sql_out[\"sql\"], \"result\": sql_out[\"polished\"], \"raw_df\": sql_out[\"raw\"]}\n",
    "    else:\n",
    "        return {\"route\":\"unknown\", \"answer\":\"Could not route the query.\"}\n",
    "\n",
    "# Try examples\n",
    "q_pdf = \"How does the claims filing process work according to the policy?\"\n",
    "q_sql = \"Show total paid amount per policy for claims opened in 2024\"\n",
    "\n",
    "print('--- Traditional RAG (naive) for PDF question ---')\n",
    "t1 = traditional_rag(q_pdf)\n",
    "print(t1[\"context\"][:800])\n",
    "\n",
    "print('\\\\n--- Agentic RAG for PDF question ---')\n",
    "a1 = agentic_rag(q_pdf)\n",
    "print('Routed to:', a1['route'])\n",
    "print(a1['answer'][:800])\n",
    "\n",
    "print('\\\\n--- Agentic RAG for SQL question ---')\n",
    "a2 = agentic_rag(q_sql)\n",
    "print('Routed to:', a2['route'])\n",
    "print('SQL used:\\\\n', a2.get('sql','(n/a)'))\n",
    "print('Result summary:\\\\n', a2.get('result')[:800] if isinstance(a2.get('result'), str) else a2.get('result').head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba672b",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Teaching notes & what to explain to your junior\n",
    "\n",
    "- **Why separate indexes?** PDFs vs SQL schema have different chunking and retrieval needs. Keeping them separate avoids noisy matches and allows tailored chunking/embedding.\n",
    "- **Schema-first approach:** SQL agent first retrieves *schema snippets* — this prevents hallucinated column/table names and enables safe SQL generation.\n",
    "- **Safety:** Always validate generated SQL (allowlist SELECTs, use read-only roles, run EXPLAIN or dry-run). For production, prefer a SQL generator LLM prompt that restricts to given schema and forbids destructive statements.\n",
    "- **Polish step:** Never send raw DB rows to users; send a human-friendly summary. The polish step reduces cognitive load and provides context.\n",
    "- **Extensibility:** You can add more indexing spaces (images, code repos, APIs) and extend the router to more classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc8c22a",
   "metadata": {},
   "source": [
    "## 14) Save & download\n",
    "\n",
    "The notebook saves the demo DB and any generated outputs under `/mnt/data`. You can download this notebook file from the environment."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
